import os
import json
import time
import random
import re
import pandas as pd
from playwright.sync_api import sync_playwright
from deep_translator import GoogleTranslator
import matplotlib.pyplot as plt
from tqdm import tqdm

# ==============================================================================
#                               æ ¸å¿ƒé…ç½®åŒºåŸŸ
# ==============================================================================

# 1. æœç´¢è®¾ç½®

# KEYWORDS = [
#     "Few-shot learning",
#     "Supervised Contrastive Learning",
#     "Prompt Tuning",
#     "5G Core Network",
#     "Threat Detection",
#     "Traffic Classification",
#     "Encrypted Traffic Analysis",
#     "Prompt-based Learning"
# ]

KEYWORDS = [
    "5G-NIDD"
]

TARGET_COUNT_PER_KEYWORD = 100   # ğŸ¯ æ¯ä¸ªå…³é”®è¯æƒ³è¦æŠ“å–çš„æ•°é‡

# 2. ç½‘ç»œä¸ä»£ç†è®¾ç½®
# æ³¨æ„ï¼šå¦‚æœä½ çš„ä»£ç†ä¸éœ€è¦ï¼Œè¯·å°† PROXY_SERVER è®¾ä¸º None
PROXY_SERVER = "http://127.0.0.1:7897"   
PROXIES = {"http": PROXY_SERVER, "https": PROXY_SERVER} if PROXY_SERVER else None

# 3. è¾“å‡ºè·¯å¾„è®¾ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, 'output')
RAW_DATA_FILE = os.path.join(OUTPUT_DIR, 'multi_keyword_data.json')
REPORT_FILE = os.path.join(OUTPUT_DIR, 'multi_keyword_report.xlsx')
CHART_FILE = os.path.join(OUTPUT_DIR, 'multi_keyword_chart.png')

# è‡ªåŠ¨åˆ›å»ºæµè§ˆå™¨ç¼“å­˜ç›®å½•ï¼Œç”¨äºä¿å­˜ç™»å½•çŠ¶æ€
USER_DATA_DIR = os.path.join(os.getcwd(), "user_data_browser")

# 4. å…¶ä»–çˆ¬è™«å‚æ•°
TIMEOUT_MS = 60000          # é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´
MIN_SLEEP = 2.0             # æœ€å°é—´éš”(ç§’)
MAX_SLEEP = 5.0             # æœ€å¤§é—´éš”(ç§’)

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ==============================================================================
#                  æ¨¡å— 1: è¾…åŠ©å‡½æ•° (æå–ã€æ£€æµ‹ã€ç¿»è¯‘)
# ==============================================================================

def extract_details(page, url):
    """ 
    å¢å¼ºç‰ˆè¯¦æƒ…æå–ï¼šæ”¯æŒ ArXiv, CVF, NeurIPS, Springer, ACM, IEEE ç­‰ä¸»æµæ¥æº 
    """
    domain = url.lower()
    content = ""
    doi = ""
    
    # --- 1. DOI æå–é€»è¾‘ ---
    try:
        doi_selectors = [
            'meta[name="citation_doi"]', 'meta[name="dc.identifier"]', 
            'meta[name="prism.doi"]', 'meta[property="og:url"]'
        ]
        for sel in doi_selectors:
            meta = page.query_selector(sel)
            if meta:
                val = meta.get_attribute("content")
                if val and "10." in val:
                    match = re.search(r'(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', val, re.IGNORECASE)
                    if match: 
                        doi = match.group(1)
                        break
        if not doi:
            match = re.search(r'(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', url, re.IGNORECASE)
            if match: doi = match.group(1)
    except: pass

    # --- 2. æ‘˜è¦æå–é€»è¾‘ ---
    try:
        if "arxiv.org" in domain:
            elem = page.query_selector("blockquote.abstract")
            if elem: content = elem.inner_text().replace("Abstract:", "").strip()
        elif "thecvf.com" in domain:
            elem = page.query_selector("div#abstract")
            if elem: content = elem.inner_text().strip()
        elif "proceedings.neurips.cc" in domain or "proceedings.mlr.press" in domain:
            elem = page.query_selector("div.abstract, .abstract-container, p.abstract")
            if not elem:
                try: content = page.locator("h4:text('Abstract') + p").inner_text()
                except: pass
            else: content = elem.inner_text()
        elif "springer.com" in domain or "nature.com" in domain:
            elem = page.query_selector("#Abs1-content, .c-article-section__content, .abstract-content")
            if elem: content = elem.inner_text()
        elif "sciencedirect.com" in domain:
            elem = page.query_selector("div.abstract.author, div#abstracts")
            if elem: content = elem.inner_text()
        elif "ieee.org" in domain:
            elem = page.query_selector("div.abstract-text, div.u-mb-1 div")
            if elem: 
                text = elem.inner_text().strip()
                if text.lower().startswith("abstract"): text = text[8:].strip(" :")
                content = text
        elif "acm.org" in domain:
            elem = page.query_selector(".abstractSection, div[role='paragraph']")
            if elem: content = elem.inner_text()
        elif "openreview.net" in domain:
            elem = page.query_selector("span.note-content-value")
            if elem: content = elem.inner_text()

        # --- 3. é€šç”¨å…œåº• ---
        if not content or len(content) < 50:
            meta_desc = page.query_selector('meta[name="description"]') or page.query_selector('meta[property="og:description"]')
            if meta_desc:
                desc_text = meta_desc.get_attribute('content').strip()
                if len(desc_text) > 50 and "10." not in desc_text[:20]: content = desc_text
            
            if not content:
                try:
                    body_text = page.inner_text("body")
                    idx = body_text.find("Abstract")
                    if idx != -1:
                        snippet = body_text[idx:idx+1500]
                        lines = [line.strip() for line in snippet.split('\n') if len(line.strip()) > 50]
                        if lines: content = lines[0]
                except: pass

    except Exception as e:
        print(f"Error parsing {url}: {e}")

    # --- 4. æ¸…æ´— ---
    if content:
        content = re.sub(r'\s+', ' ', content).strip()
        if content.lower().startswith("abstract"): content = content[8:].strip(" :-")
            
    if len(content) < 20 or content.startswith("http") or content.startswith("10."):
        content = "æœªæ‰¾åˆ°æœ‰æ•ˆæ‘˜è¦"

    return content, doi

def check_google_captcha_blocking(page):
    """ Google åçˆ¬æ‹¦æˆªæ£€æµ‹ """
    try:
        if "/sorry/" in page.url: is_blocked = True
        else:
            text = page.inner_text("body").lower()
            is_blocked = "unusual traffic" in text or "å¼‚å¸¸æµé‡" in text or "robot" in page.title().lower()

        if is_blocked:
            print("\nğŸš¨ğŸš¨ğŸš¨ è§¦å‘ Google æ‹¦æˆªï¼(æ£€æµ‹åˆ°å¼‚å¸¸æµé‡)")
            print("1. è¯·åœ¨è‡ªåŠ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ç ã€‚")
            print("2. å®Œæˆåï¼Œè¯·åœ¨ç»ˆç«¯æŒ‰ã€å›è½¦ã€‘ç»§ç»­ç¨‹åºã€‚")
            page.bring_to_front() # æŠŠé¡µé¢ç½®é¡¶
            input() 
            return True
    except: pass
    return False

def is_target_captcha(page):
    """ ç›®æ ‡è®ºæ–‡ç½‘ç«™çš„åçˆ¬æ£€æµ‹ (Cloudflareç­‰) """
    try:
        title = page.title().lower()
        body = page.inner_text("body").lower()
        if "just a moment" in title or "verify you are human" in title or "captcha" in body:
            return True
    except: pass
    return False

def rate_venue(venue_text):
    """ æœŸåˆŠè¯„çº§ """
    if not isinstance(venue_text, str): return "æœªçŸ¥", "æœªçŸ¥"
    venue_lower = venue_text.lower()
    clean_name = venue_text
    try:
        parts = venue_text.split(" - ")
        if len(parts) >= 2:
            clean_name = parts[-2]
            clean_name = re.sub(r'\d{4}', '', clean_name).strip().strip(',')
    except: pass

    level = "æ™®é€š"
    if "ieee trans" in venue_lower or "acm trans" in venue_lower: level = "é¡¶åˆŠ (Trans)"
    elif "nature" in venue_lower or "science" in venue_lower: level = "ç¥åˆŠ"
    elif any(x in venue_lower for x in ["cvpr", "iccv", "eccv", "neurips", "icml", "aaai"]): level = "é¡¶ä¼š (CCF A)"
    elif "arxiv" in venue_lower: level = "é¢„å°æœ¬ (ArXiv)"

    return clean_name, level

def translate_text(text, target='zh-CN'):
    """ è°·æ­Œç¿»è¯‘ """
    if not text or len(text) < 5 or text == "æœªæ‰¾åˆ°" or text == "æœªæ‰¾åˆ°æœ‰æ•ˆæ‘˜è¦": return ""
    try:
        translator = GoogleTranslator(source='auto', target=target, proxies=PROXIES)
        res = translator.translate(text[:4000])
        time.sleep(0.2)
        return res
    except: return "[ç¿»è¯‘å‡ºé”™]"

# ==============================================================================
#                  æ¨¡å— 2: æ ¸å¿ƒçˆ¬è™«æ§åˆ¶æµç¨‹
# ==============================================================================

def run_multi_keyword_spider():
    print(f"\nğŸš€ [é˜¶æ®µ 1/3] æ­£åœ¨å¯åŠ¨ç‹¬ç«‹æµè§ˆå™¨å®ä¾‹...")
    print(f"ğŸ“‹ å¾…æŠ“å–å…³é”®è¯åˆ—è¡¨: {KEYWORDS}")
    
    global_task_list = [] 
    seen_urls = set()     

    with sync_playwright() as p:
        # ==================== ä¿®æ”¹éƒ¨åˆ†ï¼šè‡ªåŠ¨å¯åŠ¨æµè§ˆå™¨ ====================
        try:
            # å‡†å¤‡å¯åŠ¨å‚æ•°
            launch_args = [
                "--disable-blink-features=AutomationControlled", # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
                "--no-sandbox",
                "--disable-infobars",
                "--start-maximized" # æœ€å¤§åŒ–çª—å£
            ]
            
            # é…ç½®ä»£ç† (å¦‚æœè®¾ç½®äº†)
            proxy_config = {"server": PROXY_SERVER} if PROXY_SERVER else None

            print(f"   ğŸ“‚ ä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•: {USER_DATA_DIR}")
            
            # ä½¿ç”¨ launch_persistent_context å¯åŠ¨ä¸€ä¸ªæŒä¹…åŒ–çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡
            # è¿™æ ·å¯ä»¥ä¿å­˜ä½ çš„ç™»å½•çŠ¶æ€ (Cookies)ï¼Œå‡å°‘éªŒè¯ç 
            context = p.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=False,  # å¿…é¡»ä¸º False æ‰èƒ½çœ‹åˆ°ç•Œé¢
                proxy=proxy_config,
                args=launch_args,
                viewport=None # ç¦ç”¨é»˜è®¤è§†å£å¤§å°ï¼Œè·Ÿéšçª—å£
            )
            
            # è·å–ç¬¬ä¸€ä¸ªé¡µé¢
            page = context.pages[0] if context.pages else context.new_page()
            print("   âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼")
            
            # å¯ä»¥åœ¨è¿™é‡Œæ³¨å…¥ä¸€æ®µ JS å»é™¤ webdriver ç‰¹å¾
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                })
            """)

        except Exception as e:
            print(f"   âŒ å¯åŠ¨å¤±è´¥: {e}")
            return []
        # ===============================================================

        # 2. è®¿é—® Google Scholar
        print("   ğŸ” æ­£åœ¨è®¿é—® Google Scholar...")
        try:
            page.goto("https://scholar.google.com", timeout=TIMEOUT_MS)
            
            # æ£€æµ‹æ˜¯å¦è¢«æ‹¦æˆª
            if check_google_captcha_blocking(page):
                pass # å·²ç»åœ¨å‡½æ•°é‡Œ wait for input äº†
            
            # æ£€æŸ¥æ˜¯å¦ç™»å½•ï¼ˆå¯é€‰ï¼‰
            if page.query_selector("a#gs_hdr_act_s"):
                print("   ğŸ‘‰ æç¤º: ä½ å½“å‰ä¼¼ä¹ã€æœªç™»å½•ã€‘Google è´¦å·ã€‚å»ºè®®ç™»å½•ä»¥è·å–æ›´å¤šæœç´¢ç»“æœã€‚")
            else:
                print("   ğŸ‘¤ æ£€æµ‹åˆ°å·²ç™»å½•çŠ¶æ€ (Cookiesç”Ÿæ•ˆä¸­)")

        except Exception as e:
            print(f"   âš ï¸  è®¿é—®è­¦å‘Š: {e}")

        # ================= LOOP 1: éå†æ‰€æœ‰å…³é”®è¯ (æŠ“å–åˆ—è¡¨) =================
        print(f"\nğŸŒŠ [å­é˜¶æ®µ A] å¼€å§‹éå†å…³é”®è¯...")
        
        for kw_index, keyword in enumerate(KEYWORDS):
            print(f"\n   ğŸ‘‰ ({kw_index+1}/{len(KEYWORDS)}) æ­£åœ¨æœç´¢: [{keyword}]")
            
            current_kw_count = 0
            current_offset = 0
            
            while current_kw_count < TARGET_COUNT_PER_KEYWORD:
                list_url = f"https://scholar.google.com/scholar?q={keyword.replace(' ', '+')}&start={current_offset}"
                
                retry = 0
                while retry < 3:
                    try:
                        page.goto(list_url, timeout=TIMEOUT_MS)
                        # æ£€æµ‹éªŒè¯ç 
                        if check_google_captcha_blocking(page):
                            pass 
                        page.wait_for_selector("div.gs_r", timeout=30000)
                        break
                    except:
                        retry += 1
                        time.sleep(3)
                
                cards = page.query_selector_all("div.gs_r.gs_or.gs_scl")
                if not cards: 
                    print("      âš ï¸  æœªæ‰¾åˆ°æ›´å¤šç»“æœå¡ç‰‡ï¼Œç»“æŸå½“å‰å…³é”®è¯æœç´¢ã€‚")
                    break

                exclude_ext = ('.pdf', '.gz', '.ps', '.zip')
                
                new_items_on_page = 0
                for item in cards:
                    if current_kw_count >= TARGET_COUNT_PER_KEYWORD: break
                    link_el = item.query_selector("h3.gs_rt a")
                    title_el = item.query_selector("h3.gs_rt")
                    pub_el = item.query_selector("div.gs_a")
                    
                    if link_el and title_el:
                        url = link_el.get_attribute("href")
                        if url and url.startswith("http") and not url.lower().endswith(exclude_ext):
                            if url in seen_urls: continue
                            seen_urls.add(url)
                            
                            venue, year = "Unknown", "Unknown"
                            raw_info = pub_el.inner_text() if pub_el else ""
                            try:
                                parts = raw_info.split(" - ")
                                if len(parts) >= 2:
                                    venue = parts[-2]
                                    year_match = re.search(r'\b(19|20)\d{2}\b', venue)
                                    if year_match: year = year_match.group(0)
                            except: pass
                            
                            global_task_list.append({
                                "keyword": keyword,
                                "title": title_el.inner_text(), 
                                "url": url, 
                                "venue": venue, 
                                "year": year
                            })
                            current_kw_count += 1
                            new_items_on_page += 1

                print(f"      ---> æœ¬é¡µæ–°å¢: {new_items_on_page} | è¿›åº¦: {current_kw_count}/{TARGET_COUNT_PER_KEYWORD}")
                current_offset += 10
                if current_kw_count < TARGET_COUNT_PER_KEYWORD:
                    sleep_time = random.uniform(3, 6)
                    time.sleep(sleep_time)
            
            time.sleep(random.uniform(4, 8))

        print(f"\nğŸ“‹ åˆ—è¡¨é‡‡é›†å®Œæ¯•ï¼å…± {len(global_task_list)} ç¯‡ã€‚")

        # ================= LOOP 2: éå†ä»»åŠ¡æ±  (æŠ“å–è¯¦æƒ…) =================
        print(f"\nğŸ•µï¸  [å­é˜¶æ®µ B] å¼€å§‹æ·±åº¦æŠ“å–è¯¦æƒ…...")
        
        final_results = []
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºæ€»è¿›åº¦
        for index, task in enumerate(tqdm(global_task_list, desc="Deep Crawling")):
            abstract, doi = "æœªæ‰¾åˆ°", "æœªæ‰¾åˆ°"
            try:
                page.goto(task['url'], timeout=45000, wait_until="domcontentloaded")
                time.sleep(random.uniform(2.0, 4.0)) 
                
                if is_target_captcha(page):
                    abstract = "éªŒè¯ç æ‹¦æˆª (å·²è·³è¿‡)"
                else:
                    abstract, doi = extract_details(page, task['url'])
            except Exception:
                abstract = "è®¿é—®å¼‚å¸¸"
            
            # å°†ç»“æœåŠ å…¥åˆ—è¡¨
            task_result = {**task, "doi": doi, "abstract": abstract}
            final_results.append(task_result)

            # æ¯çˆ¬ 10 ç¯‡è‡ªåŠ¨ä¿å­˜
            if (index + 1) % 10 == 0:
                try:
                    with open(RAW_DATA_FILE, 'w', encoding='utf-8') as f:
                        json.dump(final_results, f, ensure_ascii=False, indent=4)
                except: pass

        # å…³é—­æµè§ˆå™¨ä¸Šä¸‹æ–‡
        try:
            context.close()
        except: pass
        
    return final_results

# ==============================================================================
#                  æ¨¡å— 3: æ•°æ®åˆ†æä¸ç¿»è¯‘
# ==============================================================================

def run_analyzer_module(data_list):
    print(f"\nğŸ“Š [é˜¶æ®µ 2/3] å¯åŠ¨æ•°æ®å¤„ç†ä¸ç¿»è¯‘...")
    
    if not data_list:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æï¼")
        return

    df = pd.DataFrame(data_list)
    
    # 1. è¯„çº§
    print("   ğŸ·ï¸  æ­£åœ¨è¿›è¡ŒæœŸåˆŠåˆ†çº§...")
    df[['Clean_Venue', 'Level']] = df['venue'].apply(lambda x: pd.Series(rate_venue(x)))

    # 2. ç¿»è¯‘ (å¸¦è¿›åº¦æ¡)
    print("   ğŸŒ æ­£åœ¨ç¿»è¯‘æ ‡é¢˜ä¸æ‘˜è¦ (è°ƒç”¨ Google API)...")
    # å¦‚æœæ²¡æœ‰é…ç½®ä»£ç†ï¼Œä¸”å›½å†…ç½‘ç»œç¯å¢ƒå·®ï¼Œè¿™é‡Œå¯èƒ½ä¼šæŠ¥é”™
    tqdm.pandas(desc="Translating")
    df['æ ‡é¢˜(ä¸­æ–‡)'] = df['title'].progress_apply(lambda x: translate_text(x))
    df['æ‘˜è¦(ä¸­æ–‡)'] = df['abstract'].progress_apply(lambda x: translate_text(x))

    # 3. æ•´ç†åˆ—é¡ºåº
    cols = ['keyword', 'title', 'æ ‡é¢˜(ä¸­æ–‡)', 'Level', 'Clean_Venue', 'year', 'doi', 'url', 'abstract', 'æ‘˜è¦(ä¸­æ–‡)']
    final_cols = [c for c in cols if c in df.columns]
    
    # 4. ä¿å­˜
    try:
        # ä¿å­˜åŸå§‹ JSON
        with open(RAW_DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, ensure_ascii=False, indent=4)
            
        # ä¿å­˜ Excel
        df[final_cols].to_excel(REPORT_FILE, index=False)
        print(f"   ğŸ’¾ æ•°æ®å·²ä¿å­˜:\n      -> Excel: {REPORT_FILE}\n      -> JSON:  {RAW_DATA_FILE}")
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")

    # 5. ç®€å•ç»˜å›¾
    try:
        if not df.empty:
            plt.figure(figsize=(12, 6))
            # ç»Ÿè®¡å„ä¸ªå…³é”®è¯çš„å¹´ä»½åˆ†å¸ƒ
            df_filtered = df[df['year'].astype(str).str.match(r'^\d{4}$')]
            if not df_filtered.empty:
                df_filtered.groupby(['year', 'keyword']).size().unstack().plot(kind='bar', stacked=True)
                plt.title('Paper Count by Year & Keyword')
                plt.savefig(CHART_FILE)
                print(f"   ğŸ“Š ç»Ÿè®¡å›¾è¡¨å·²ç”Ÿæˆ: {CHART_FILE}")
    except Exception as e: 
        print(f"ç»˜å›¾è·³è¿‡: {e}")

# ==============================================================================
#                  ä¸»ç¨‹åºå…¥å£
# ==============================================================================

if __name__ == "__main__":
    print("="*60)
    print(f"ğŸš€  Advanced Multi-Keyword Scholar Pipeline (Auto-Launch)")
    print(f"ğŸ“‚  Working Dir: {BASE_DIR}")
    print("="*60)
    
    # 1. æ‰§è¡Œçˆ¬è™« (åˆ—è¡¨ -> è¯¦æƒ…)
    raw_data = run_multi_keyword_spider()
    
    # 2. æ‰§è¡Œåˆ†æ (ç¿»è¯‘ -> æŠ¥è¡¨)
    if raw_data:
        run_analyzer_module(raw_data)
    
    print("\nğŸ‰ğŸ‰ğŸ‰ å…¨æµç¨‹ä»»åŠ¡å®Œæˆï¼")