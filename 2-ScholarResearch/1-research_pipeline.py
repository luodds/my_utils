import os
import json
import time
import random
import re
import pandas as pd
from playwright.sync_api import sync_playwright
from deep_translator import GoogleTranslator
import matplotlib.pyplot as plt
from tqdm import tqdm

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ (åªéœ€æ”¹è¿™é‡Œ) =================

# 1. æœç´¢è®¾ç½®
KEYWORD = "Few-shot learning"    # ğŸ” åœ¨è¿™é‡Œä¿®æ”¹ä½ æƒ³æœç´¢çš„å…³é”®è¯
TARGET_COUNT = 5               # ğŸ¯ æƒ³è¦æŠ“å–çš„è®ºæ–‡æ•°é‡ (æµ‹è¯•å»ºè®®å…ˆå¡« 20-50)

# 2. ç½‘ç»œä¸ä»£ç†
PROXY_SERVER = "http://127.0.0.1:2011" # ğŸŒ ä½ çš„æœ¬åœ°ä»£ç†åœ°å€
PROXIES = {"http": PROXY_SERVER, "https": PROXY_SERVER}

# 3. è·¯å¾„è®¾ç½® (è‡ªåŠ¨è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, 'output')
RAW_DATA_FILE = os.path.join(OUTPUT_DIR, '1-raw_data.json')
REPORT_FILE = os.path.join(OUTPUT_DIR, '1-analysis_report.xlsx')
CHART_FILE = os.path.join(OUTPUT_DIR, '1-trend_chart.png')
USER_DATA_DIR = os.path.join(os.getcwd(), "user_data_browser")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ==========================================================
#                  æ¨¡å— 1: çˆ¬è™«é€»è¾‘ (Spider)
# ==========================================================

def extract_details(page, url):
    """ è¯¦æƒ…é¡µæå–æ‘˜è¦å’ŒDOI """
    domain = url.lower()
    content = ""
    doi = ""
    try:
        # DOI æå–
        doi_meta = (page.query_selector('meta[name="citation_doi"]') or 
                    page.query_selector('meta[name="dc.identifier"]') or 
                    page.query_selector('meta[name="prism.doi"]'))
        if doi_meta: doi = doi_meta.get_attribute("content").strip()
        if not doi:
            doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', url, re.IGNORECASE)
            if doi_match: doi = doi_match.group(1)

        # æ‘˜è¦æå–
        if "sciencedirect.com" in domain:
            try:
                script = page.eval_on_selector("script[type='application/ld+json']", "el => el.innerText")
                data = json.loads(script)
                if 'description' in data: content = data['description']
                if not doi and 'identifier' in data: doi = data['identifier']
            except: pass
            if not content:
                elem = page.query_selector("div.abstract") or page.query_selector("div[id^='abs']")
                if elem: content = elem.inner_text().strip()
        elif "ieee.org" in domain:
            try: page.wait_for_selector("div.abstract-text", timeout=3000)
            except: pass
            elem = page.query_selector("div.abstract-text")
            if elem: 
                text = elem.inner_text().strip()
                if text.lower().startswith("abstract"): text = text.split(":", 1)[-1].strip()
                content = text
        
        if not content:
            meta = page.query_selector('meta[name="description"]') or page.query_selector('meta[property="og:description"]')
            if meta: content = meta.get_attribute('content').strip()

    except Exception: pass
    return content, doi

def check_google_captcha_blocking(page):
    """ åˆ—è¡¨é¡µåçˆ¬æ£€æµ‹ (å·²å¢å¼ºå¯¹ä¸­æ–‡å’ŒURLçš„æ£€æµ‹) """
    try:
        # 1. æ£€æŸ¥ URL æ˜¯å¦åŒ…å« /sorry/ (Google æ‹¦æˆªé¡µçš„ç‰¹å¾)
        if "/sorry/" in page.url:
            is_blocked = True
        else:
            # 2. æ£€æŸ¥é¡µé¢æ–‡å­—å…³é”®è¯
            text = page.inner_text("body").lower()
            is_blocked = "unusual traffic" in text or "å¼‚å¸¸æµé‡" in text or "robot" in page.title().lower()

        if is_blocked:
            print("\nğŸš¨ğŸš¨ğŸš¨ è§¦å‘ Google æ‹¦æˆªï¼(æ£€æµ‹åˆ°å¼‚å¸¸æµé‡)")
            print("ğŸ‘‡ åŠ¨ä½œæŒ‡å¯¼ï¼š")
            print("1. è¯·åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹æ˜¯å¦æœ‰ã€éªŒè¯ç /å¤é€‰æ¡†ã€‘ã€‚")
            print("2. å¦‚æœæœ‰ï¼Œè¯·æ‰‹åŠ¨ç‚¹å‡»å¹¶å®ŒæˆéªŒè¯ï¼Œç›´åˆ°çœ‹åˆ°æ­£å¸¸çš„æœç´¢åˆ—è¡¨ã€‚")
            print("3. å¦‚æœæ²¡æœ‰éªŒè¯ç ï¼ˆç™½å±æˆ–çº¯æ–‡å­—ï¼‰ï¼Œè¯´æ˜ IP è¢«å°ï¼Œè¯·æ›´æ¢ä»£ç†èŠ‚ç‚¹æˆ–ç¨åå†è¯•ã€‚")
            print("waiting... (å®Œæˆæ“ä½œåï¼Œè¯·åœ¨ç»ˆç«¯æŒ‰ã€å›è½¦ã€‘ç»§ç»­)")
            
            # è¿™é‡Œçš„ input ä¼šæš‚åœç¨‹åºï¼Œç­‰ä½ å¤„ç†å®Œæµè§ˆå™¨
            input() 
            return True
    except: pass
    return False

def is_target_captcha(page):
    """ è¯¦æƒ…é¡µåçˆ¬æ£€æµ‹ (è‡ªåŠ¨è·³è¿‡) """
    try:
        title = page.title().lower()
        body = page.inner_text("body").lower()
        if "just a moment" in title or "verify you are human" in title or "captcha" in body or "are you a robot" in body:
            return True
    except: pass
    return False

def run_spider_module():
    print(f"\nğŸ•·ï¸  [é˜¶æ®µ 1/2] å¯åŠ¨çˆ¬è™« | å…³é”®è¯: {KEYWORD}")
    task_list = [] 

    with sync_playwright() as p:
        context = p.chromium.launch_persistent_context(
            user_data_dir=USER_DATA_DIR,
            headless=False, 
            proxy={"server": PROXY_SERVER},
            args=['--disable-blink-features=AutomationControlled'], 
            viewport={"width": 1600, "height": 900},
        )
        page = context.pages[0]

        # --- å­é˜¶æ®µ 1: åˆ—è¡¨æ‰«æ ---
        current_offset = 0
        print(f"   ğŸ“– æ­£åœ¨æ‰«æåˆ—è¡¨é¡µ...")
        
        while len(task_list) < TARGET_COUNT:
            list_url = f"https://scholar.google.com/scholar?q={KEYWORD.replace(' ', '+')}&start={current_offset}"
            
            retry = 0
            while retry < 3:
                try:
                    page.goto(list_url, timeout=60000)
                    check_google_captcha_blocking(page)
                    page.wait_for_selector("div.gs_r", timeout=30000)
                    break
                except:
                    retry += 1
                    time.sleep(3)
            
            cards = page.query_selector_all("div.gs_r.gs_or.gs_scl")
            if not cards: break

            exclude = ('.pdf', '.gz', '.ps', '.zip')
            for item in cards:
                if len(task_list) >= TARGET_COUNT: break
                link_el = item.query_selector("h3.gs_rt a")
                title_el = item.query_selector("h3.gs_rt")
                pub_el = item.query_selector("div.gs_a")
                
                if link_el and title_el:
                    url = link_el.get_attribute("href")
                    if url and url.startswith("http") and not url.lower().endswith(exclude):
                        venue, year = "Unknown", "Unknown"
                        raw_info = pub_el.inner_text() if pub_el else ""
                        try:
                            parts = raw_info.split(" - ")
                            if len(parts) >= 2:
                                venue = parts[-2]
                                year_match = re.search(r'\b(19|20)\d{2}\b', venue)
                                if year_match: year = year_match.group(0)
                        except: pass
                        task_list.append({"title": title_el.inner_text(), "url": url, "venue": venue, "year": year})

            current_offset += 10
            print(f"      ---> å·²æ”¶é›†: {len(task_list)}/{TARGET_COUNT}")
            if len(task_list) < TARGET_COUNT:
                time.sleep(random.uniform(2, 4))

        # --- å­é˜¶æ®µ 2: è¯¦æƒ…æŠ“å– ---
        print(f"\n   ğŸ•µï¸  æ­£åœ¨æŠ“å–è¯¦æƒ… (æ‘˜è¦ & DOI)...")
        final_results = []
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for task in tqdm(task_list, desc="Deep Crawling"):
            abstract, doi = "æœªæ‰¾åˆ°", "æœªæ‰¾åˆ°"
            try:
                page.goto(task['url'], timeout=15000, wait_until="domcontentloaded")
                time.sleep(random.uniform(1.0, 2.5))
                
                if is_target_captcha(page):
                    abstract = "éªŒè¯ç æ‹¦æˆª (å·²è·³è¿‡)"
                else:
                    abstract, doi = extract_details(page, task['url'])
            except Exception:
                abstract = "è®¿é—®å¼‚å¸¸"
            
            final_results.append({**task, "doi": doi, "abstract": abstract})
        
        context.close()
    
    # ä¿å­˜åŸå§‹æ•°æ®
    with open(RAW_DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=4)
    print(f"   âœ… çˆ¬è™«ç»“æŸï¼Œæ•°æ®å·²ä¿å­˜è‡³: {RAW_DATA_FILE}")
    return final_results

# ==========================================================
#                  æ¨¡å— 2: åˆ†æé€»è¾‘ (Analyzer)
# ==========================================================

tqdm.pandas() 

def rate_venue(venue_text):
    """ æœŸåˆŠè¯„çº§é€»è¾‘ """
    if not isinstance(venue_text, str): return "æœªçŸ¥", "æœªçŸ¥"
    venue_lower = venue_text.lower()
    clean_name = venue_text
    try:
        parts = venue_text.split(" - ")
        if len(parts) >= 2:
            clean_name = parts[-2]
            clean_name = re.sub(r'\d{4}', '', clean_name).strip().strip(',')
    except: pass

    level = "æ™®é€š"
    if "ieee trans" in venue_lower or "acm trans" in venue_lower: level = "é¡¶åˆŠ (Trans)"
    elif "nature" in venue_lower or "science" in venue_lower: level = "ç¥åˆŠ"
    elif any(x in venue_lower for x in ["cvpr", "iccv", "eccv", "neurips", "icml", "aaai"]): level = "é¡¶ä¼š (CCF A)"
    elif "arxiv" in venue_lower: level = "é¢„å°æœ¬ (ArXiv)"

    return clean_name, level

def translate_text(text, target='zh-CN'):
    """ ç¿»è¯‘å‡½æ•° """
    if not text or len(text) < 5 or text == "æœªæ‰¾åˆ°": return ""
    try:
        translator = GoogleTranslator(source='auto', target=target, proxies=PROXIES)
        res = translator.translate(text[:4000])
        time.sleep(0.2)
        return res
    except: return "[ç¿»è¯‘å‡ºé”™]"

def run_analyzer_module(data_list):
    print(f"\nğŸ“Š [é˜¶æ®µ 2/2] å¯åŠ¨åˆ†æä¸ç¿»è¯‘...")
    
    if not data_list:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æï¼")
        return

    df = pd.DataFrame(data_list)
    
    # 1. è¯„çº§
    print("   ğŸ·ï¸  æ­£åœ¨è¿›è¡ŒæœŸåˆŠåˆ†çº§...")
    df[['Clean_Venue', 'Level']] = df['venue'].apply(lambda x: pd.Series(rate_venue(x)))

    # 2. ç¿»è¯‘ (å¸¦è¿›åº¦æ¡)
    print("   ğŸŒ æ­£åœ¨ç¿»è¯‘æ ‡é¢˜ä¸æ‘˜è¦ (è°ƒç”¨ Google API)...")
    print("      (å¦‚æœå¡ä½è¯·æ£€æŸ¥ä»£ç†æ˜¯å¦ç¨³å®š)")
    df['æ ‡é¢˜(ä¸­æ–‡)'] = df['title'].progress_apply(lambda x: translate_text(x))
    df['æ‘˜è¦(ä¸­æ–‡)'] = df['abstract'].progress_apply(lambda x: translate_text(x))

    # 3. ä¿å­˜ Excel
    cols = ['title', 'æ ‡é¢˜(ä¸­æ–‡)', 'Level', 'Clean_Venue', 'year', 'doi', 'url', 'abstract', 'æ‘˜è¦(ä¸­æ–‡)']
    final_cols = [c for c in cols if c in df.columns]
    
    try:
        df[final_cols].to_excel(REPORT_FILE, index=False)
        print(f"   ğŸ’¾ Excel æŠ¥è¡¨å·²ç”Ÿæˆ: {REPORT_FILE}")
    except Exception as e:
        print(f"   âŒ ä¿å­˜ Excel å¤±è´¥ (è¯·å…³é—­æ–‡ä»¶é‡è¯•): {e}")

    # 4. ç»˜å›¾
    try:
        year_counts = df['year'].value_counts().sort_index()
        year_counts = year_counts[year_counts.index.str.match(r'^\d{4}$', na=False)]
        if not year_counts.empty:
            plt.figure(figsize=(10, 6))
            year_counts.plot(kind='bar', color='skyblue')
            plt.title(f'Publication Trend: {KEYWORD}')
            plt.savefig(CHART_FILE)
            print(f"   ğŸ“Š è¶‹åŠ¿å›¾å·²ç”Ÿæˆ: {CHART_FILE}")
    except: pass

# ==========================================================
#                  ä¸»ç¨‹åºå…¥å£
# ==========================================================

if __name__ == "__main__":
    print("="*50)
    print(f"ğŸš€  Scholar Research Pipeline (One-Step)")
    print(f"ğŸ“‚  Root: {BASE_DIR}")
    print("="*50)
    
    # æ­¥éª¤ 1: çˆ¬å–æ•°æ®
    raw_data = run_spider_module()
    
    # æ­¥éª¤ 2: åˆ†ææ•°æ® (å¦‚æœçˆ¬åˆ°äº†æ•°æ®)
    if raw_data:
        run_analyzer_module(raw_data)
    
    print("\nğŸ‰ğŸ‰ğŸ‰ å…¨æµç¨‹ä»»åŠ¡å®Œæˆï¼è¯·æŸ¥çœ‹ output ç›®å½•ã€‚")