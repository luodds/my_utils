from playwright.sync_api import sync_playwright
import time
import random
import json
import os
import re

# === é…ç½®åŒºåŸŸ ===
KEYWORD = "GNNExplainer"
TARGET_COUNT = 300                      # ç›®æ ‡æ•°é‡
PROXY_SERVER = "http://127.0.0.1:2011"  
JSON_FILENAME = "2-ScholarResearch/output/1-raw_data.json"
# ==============

def extract_details(page, url):
    """ è¯¦æƒ…é¡µæå–é€»è¾‘ """
    domain = url.lower()
    content = ""
    doi = ""
    try:
        # DOI æå–
        doi_meta = (page.query_selector('meta[name="citation_doi"]') or 
                    page.query_selector('meta[name="dc.identifier"]') or 
                    page.query_selector('meta[name="prism.doi"]'))
        if doi_meta: doi = doi_meta.get_attribute("content").strip()
        if not doi:
            doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', url, re.IGNORECASE)
            if doi_match: doi = doi_match.group(1)

        # æ‘˜è¦æå–
        if "sciencedirect.com" in domain:
            try:
                script = page.eval_on_selector("script[type='application/ld+json']", "el => el.innerText")
                data = json.loads(script)
                if 'description' in data: content = data['description']
                if not doi and 'identifier' in data: doi = data['identifier']
            except: pass
            if not content:
                elem = page.query_selector("div.abstract") or page.query_selector("div[id^='abs']")
                if elem: content = elem.inner_text().strip()
        elif "ieee.org" in domain:
            try: page.wait_for_selector("div.abstract-text", timeout=3000)
            except: pass
            elem = page.query_selector("div.abstract-text")
            if elem: 
                text = elem.inner_text().strip()
                if text.lower().startswith("abstract"): text = text.split(":", 1)[-1].strip()
                content = text
        
        if not content:
            meta = page.query_selector('meta[name="description"]') or page.query_selector('meta[property="og:description"]')
            if meta: content = meta.get_attribute('content').strip()

    except Exception: pass
    return content, doi

def check_google_captcha_blocking(page):
    """ 
    ä¸“é—¨ç”¨äºã€é˜¶æ®µ1ï¼šåˆ—è¡¨é¡µã€‘çš„å¼ºåŠ›æ‹¦æˆª 
    åªæœ‰åœ¨ Google åˆ—è¡¨é¡µè¢«å°æ—¶æ‰æš‚åœï¼Œå› ä¸ºåˆ—è¡¨é¡µå¿…é¡»è§£å°æ‰èƒ½ç»§ç»­
    """
    try:
        title = page.title().lower()
        if "robot" in title or "unusual traffic" in page.inner_text("body").lower():
            print("\nğŸš¨ğŸš¨ğŸš¨ Google åˆ—è¡¨é¡µè¢«é”ï¼å¿…é¡»äººå·¥ä»‹å…¥ï¼")
            print("ğŸ‘‰ è¯·åœ¨æµè§ˆå™¨æ‰‹åŠ¨è¿‡éªŒè¯ã€‚")
            input("âœ… å®ŒæˆåæŒ‰ã€å›è½¦ã€‘ç»§ç»­...")
            return True
    except: pass
    return False

def is_target_captcha(page):
    """
    ä¸“é—¨ç”¨äºã€é˜¶æ®µ2ï¼šè¯¦æƒ…é¡µã€‘çš„æ£€æµ‹
    åªè¿”å› True/Falseï¼Œä¸æš‚åœç¨‹åº
    """
    try:
        title = page.title().lower()
        body = page.inner_text("body").lower()
        # å¸¸è§éªŒè¯ç ç‰¹å¾è¯
        if "just a moment" in title or "verify you are human" in title or "captcha" in body or "are you a robot" in body:
            return True
    except: pass
    return False

def run():
    USER_DATA_DIR = os.path.join(os.getcwd(), "user_data_browser") 
    task_list = [] 

    with sync_playwright() as p:
        print(f"ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        context = p.chromium.launch_persistent_context(
            user_data_dir=USER_DATA_DIR,
            headless=False, 
            proxy={"server": PROXY_SERVER},
            args=['--disable-blink-features=AutomationControlled'], 
            viewport={"width": 1600, "height": 900},
        )
        page = context.pages[0]

        # ==========================================
        # é˜¶æ®µ 1: æ‰«æåˆ—è¡¨é¡µ (å¿…é¡»ä¿è¯æˆåŠŸï¼Œå¦åˆ™æš‚åœ)
        # ==========================================
        print(f"\n======== é˜¶æ®µ 1: æ‰«æåˆ—è¡¨é¡µ (ç›®æ ‡: {TARGET_COUNT} ç¯‡) ========")
        
        current_offset = 0
        while len(task_list) < TARGET_COUNT:
            list_url = f"https://scholar.google.com/scholar?q={KEYWORD.replace(' ', '+')}&start={current_offset}"
            print(f"ğŸ“– æ‰«æç¬¬ {current_offset//10 + 1} é¡µ (è¿›åº¦ {len(task_list)}/{TARGET_COUNT})...")
            
            retry = 0
            while retry < 3:
                try:
                    page.goto(list_url, timeout=60000)
                    check_google_captcha_blocking(page) # è¿™é‡Œå¦‚æœè¢«å°ï¼Œä¼šæš‚åœç­‰ä½ ä¿®
                    page.wait_for_selector("div.gs_r", timeout=30000)
                    break
                except:
                    print("   âš ï¸ åˆ—è¡¨é¡µåŠ è½½æ…¢ï¼Œé‡è¯•ä¸­...")
                    retry += 1
                    time.sleep(3)
            
            cards = page.query_selector_all("div.gs_r.gs_or.gs_scl")
            if not cards: break

            new_cnt = 0
            exclude = ('.pdf', '.gz', '.ps', '.zip')
            for item in cards:
                if len(task_list) >= TARGET_COUNT: break
                link_el = item.query_selector("h3.gs_rt a")
                title_el = item.query_selector("h3.gs_rt")
                pub_el = item.query_selector("div.gs_a")
                
                if link_el and title_el:
                    url = link_el.get_attribute("href")
                    if url and url.startswith("http") and not url.lower().endswith(exclude):
                        venue, year = "Unknown", "Unknown"
                        raw_info = pub_el.inner_text() if pub_el else ""
                        try:
                            parts = raw_info.split(" - ")
                            if len(parts) >= 2:
                                venue = parts[-2]
                                year_match = re.search(r'\b(19|20)\d{2}\b', venue)
                                if year_match: year = year_match.group(0)
                        except: pass
                        
                        task_list.append({"title": title_el.inner_text(), "url": url, "venue": venue, "year": year, "raw_info": raw_info})
                        new_cnt += 1
            
            print(f"   âœ… æ–°å¢ {new_cnt} æ¡")
            current_offset += 10
            time.sleep(random.uniform(2, 5)) # åˆ—è¡¨é¡µç¿»é¡µå¿…é¡»ä¼‘æ¯

        # ==========================================
        # é˜¶æ®µ 2: æ·±åº¦æŠ“å– (é‡åˆ°éªŒè¯ç ç›´æ¥è·³è¿‡)
        # ==========================================
        print(f"\n======== é˜¶æ®µ 2: æ·±åº¦æŠ“å– (è‡ªåŠ¨è·³è¿‡éªŒè¯ç ) ========")
        final_results = []
        
        for i, task in enumerate(task_list):
            print(f"ğŸ‘‰ [{i+1}/{len(task_list)}] {task['title'][:20]}...")
            abstract, doi = "æœªæ‰¾åˆ°", "æœªæ‰¾åˆ°"
            
            try:
                # ç¼©çŸ­è¶…æ—¶æ—¶é—´ï¼Œå¦‚æœå¡ä½ç›´æ¥ç®—è·³è¿‡
                page.goto(task['url'], timeout=15000, wait_until="domcontentloaded")
                time.sleep(random.uniform(1.5, 3))
                
                # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šæ£€æµ‹æ˜¯å¦æ˜¯éªŒè¯ç é¡µé¢
                if is_target_captcha(page):
                    print(f"   ğŸ’¨ è§¦å‘éªŒè¯ç æ‹¦æˆªï¼Œè‡ªåŠ¨è·³è¿‡ (SKIP)")
                    abstract = "éªŒè¯ç æ‹¦æˆª (å·²è·³è¿‡)"
                else:
                    abstract, doi = extract_details(page, task['url'])
                    print(f"   ğŸ“ æ‘˜è¦: {len(abstract)}å­— | DOI: {doi}")

            except Exception as e:
                print(f"   âŒ è®¿é—®å¼‚å¸¸: {str(e)[:20]} (å·²è·³è¿‡)")
                abstract = "è®¿é—®å¼‚å¸¸"
            
            final_results.append({**task, "doi": doi, "abstract": abstract})
            
            if (i + 1) % 20 == 0:
                with open(JSON_FILENAME, 'w', encoding='utf-8') as f:
                    json.dump(final_results, f, ensure_ascii=False, indent=4)
                print("   ğŸ’¾ è‡ªåŠ¨ä¿å­˜...")

        context.close()

    if final_results:
        with open(JSON_FILENAME, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=4)
        print(f"\nğŸ‰ å®Œæˆï¼ç»“æœå·²ä¿å­˜: {JSON_FILENAME}")

if __name__ == "__main__":
    run()